services:
  deepseek:
    image: ghcr.io/ggerganov/llama.cpp:latest
    volumes:
      - models:/models
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        mkdir -p /models
        if [ ! -f /models/deepseek-7b.Q4_K.gguf ]; then
          echo "Downloading DeepSeek-Small (7B) quantized model..."
          wget -O /models/deepseek-7b.Q4_K.gguf https://huggingface.co/TheBloke/DeepSeek-7B-GGUF/resolve/main/deepseek-7b.Q4_K.gguf
        else
          echo "Model already exists. Skipping download."
        fi
        echo "Starting Llama.cpp server..."
        /app/server --model /models/deepseek-7b.Q4_K.gguf --host 0.0.0.0 --port 8080 --threads 8 --n-gpu-layers 0

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    volumes:
      - webui-data:/app/backend/data
    labels:
      caddy: deepseek.colman.com.br
      caddy.reverse_proxy: "{{upstreams 3000}}"
    environment:
      - WEBUI_SECRET_KEY
      - OLLAMA_API_BASE_URL=http://deepseek:8080
      - DEBUG_MODE=false
    depends_on:
      - deepseek

volumes:
  models:
  webui-data: